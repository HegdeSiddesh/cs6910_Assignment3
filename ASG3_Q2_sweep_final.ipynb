{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HegdeSiddesh/cs6910_Assignment3/blob/main/ASG3_Q2_sweep_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oWJRqX0rp2R",
        "outputId": "24a66eed-84bb-4d28-f931-8eda21af059d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 266 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 276 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 296 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 307 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 327 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 337 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 358 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 368 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 389 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 399 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 409 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 419 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 430 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 440 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 450 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 460 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 471 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 481 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 491 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 512 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 522 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 532 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 542 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 563 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 573 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 583 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 593 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 604 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 614 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 624 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 634 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 645 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 655 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 675 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 686 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 696 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 706 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 716 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 727 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 737 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 747 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 757 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 768 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 788 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 798 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 808 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 819 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 829 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 839 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 849 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 860 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 870 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 880 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 890 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 901 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 911 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 921 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 931 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 942 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 952 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 962 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 972 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 983 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 993 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.2 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.8 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 93.0 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 99.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=70c19dcae8455f72b7ef877596fc776741ad40c19ba84369c464d500d63852b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.15\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "!pip install --upgrade wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "4tUrkPJqj9MP",
        "outputId": "cbe5e9b6-4696-4b94-e875-871a0df16a41"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dlawNlDsD9W",
        "outputId": "2a9fdeda-1422-445c-ed1f-73e072802b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 06:52:40--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.99.128, 142.250.107.128, 74.125.199.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.99.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   151MB/s    in 13s     \n",
            "\n",
            "2022-04-30 06:52:53 (148 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Download and unzip the Dakshina dataset \n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf '/content/dakshina_dataset_v1.0.tar'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "e4MzdnSEkl9H",
        "outputId": "48f1b4d0-a292-4c48-cc6e-52e416ca1158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhithesh-sidhesh\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220430_065300-iewwg0oz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil/runs/iewwg0oz\" target=\"_blank\">Question_2</a></strong> to <a href=\"https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil/runs/iewwg0oz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff52b6c53d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "wandb.init(project=\"CS6910-Assignment_3-sweep-Tamil\", entity=\"hithesh-sidhesh\", name=\"Question_2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky6kbChVs-d9"
      },
      "outputs": [],
      "source": [
        "#Read the contents from the csv file. Here we have chosen english to tamil transliteration.\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil,sep='\\t',header=None,names=[\"Tamil\",\"English\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "    data = data[data['Tamil'].notna()] # using notna() ensures 'nan' are deleted from the data. If not it will throw an error while training.\n",
        "    data = data[data['English'].notna()]\n",
        "    data = data[['Tamil','English']]\n",
        "    return data\n",
        "\n",
        "train = load_data(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\")\n",
        "val = load_data(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\")\n",
        "test = load_data(\"/content/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TP489yXIV_o4"
      },
      "outputs": [],
      "source": [
        "x = train['English'].values #Get the english words alone\n",
        "y = train['Tamil'].values #Get the tamil words alone\n",
        "# As y = f(x) given , we assign english words to x and tamil words to y. We are training the seq2seq model to transliterate from english to tamil.\n",
        "\n",
        "# We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "y = \"\\t\"+y+\"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7alRIOMSF7N",
        "outputId": "4b563652-8b2e-4306-a330-61d6273d4263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 68215\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n",
            "Max sequence length for inputs: 30\n",
            "Max sequence length for outputs: 28\n",
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, ' ': 26}\n",
            "{'\\t': 0, '\\n': 1, 'ஃ': 2, 'அ': 3, 'ஆ': 4, 'இ': 5, 'ஈ': 6, 'உ': 7, 'ஊ': 8, 'எ': 9, 'ஏ': 10, 'ஐ': 11, 'ஒ': 12, 'ஓ': 13, 'க': 14, 'ங': 15, 'ச': 16, 'ஜ': 17, 'ஞ': 18, 'ட': 19, 'ண': 20, 'த': 21, 'ந': 22, 'ன': 23, 'ப': 24, 'ம': 25, 'ய': 26, 'ர': 27, 'ற': 28, 'ல': 29, 'ள': 30, 'ழ': 31, 'வ': 32, 'ஷ': 33, 'ஸ': 34, 'ஹ': 35, 'ா': 36, 'ி': 37, 'ீ': 38, 'ு': 39, 'ூ': 40, 'ெ': 41, 'ே': 42, 'ை': 43, 'ொ': 44, 'ோ': 45, 'ௌ': 46, '்': 47, ' ': 48}\n"
          ]
        }
      ],
      "source": [
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "for i in range(len(x)):\n",
        "  input_characters.update(list(str(x[i])))\n",
        "  target_characters.update(list(str(y[i])))\n",
        "\n",
        "#We get the unique characters(alphabets) present in the source and target language.\n",
        "\n",
        "  # input_characters=list(set(input_characters+list(str(x[i]))))\n",
        "  # target_characters=list(set(target_characters+list(str(y[i]))))\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "#Sorting the alphabets present in the source and target language.\n",
        "\n",
        "# add the space character to both\n",
        "input_characters.append(\" \")\n",
        "target_characters.append(\" \")\n",
        "\n",
        "#Getting the total number of alphabets present in the source and target language \n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "input_length=[]\n",
        "target_length=[]\n",
        "for i in range(len(x)):\n",
        "  input_length.append(len(str(x[i])))\n",
        "  target_length.append(len(str(y[i])))\n",
        "\n",
        "#Getting the maximum length of the word present in the source and target language\n",
        "max_encoder_seq_length = max(input_length)\n",
        "max_decoder_seq_length = max(target_length)\n",
        "\n",
        "print(\"Number of samples:\", len(x))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "\n",
        "#Making a dictionary for the alphabets(unique characters) present in the source and target language\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "print(input_token_index)\n",
        "print(target_token_index)\n",
        "\n",
        "#Defining tensor for input and output data.\n",
        "encoder_input_data = np.zeros((len(x), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(x), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(x), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "#Getting one-hot representrations for each word.\n",
        "for i, (input_text, target_text) in enumerate(zip(x, y)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and hence will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c9SsGTwlcjR"
      },
      "outputs": [],
      "source": [
        "#Embedding validation data\n",
        "x_val = val['English'].values\n",
        "y_val = val['Tamil'].values\n",
        "# We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "y_val = \"\\t\"+y_val+\"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPEuR3X2r8V8"
      },
      "outputs": [],
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJgI3viNob-6",
        "outputId": "1573c969-653e-47df-d081-9b80cd3cd522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in validation set: 6827\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 49\n",
            "Max sequence length for validation inputs: 23\n",
            "Max sequence length for valiation outputs: 22\n",
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, ' ': 26}\n",
            "{'\\t': 0, '\\n': 1, 'ஃ': 2, 'அ': 3, 'ஆ': 4, 'இ': 5, 'ஈ': 6, 'உ': 7, 'ஊ': 8, 'எ': 9, 'ஏ': 10, 'ஐ': 11, 'ஒ': 12, 'ஓ': 13, 'க': 14, 'ங': 15, 'ச': 16, 'ஜ': 17, 'ஞ': 18, 'ட': 19, 'ண': 20, 'த': 21, 'ந': 22, 'ன': 23, 'ப': 24, 'ம': 25, 'ய': 26, 'ர': 27, 'ற': 28, 'ல': 29, 'ள': 30, 'ழ': 31, 'வ': 32, 'ஷ': 33, 'ஸ': 34, 'ஹ': 35, 'ா': 36, 'ி': 37, 'ீ': 38, 'ு': 39, 'ூ': 40, 'ெ': 41, 'ே': 42, 'ை': 43, 'ொ': 44, 'ோ': 45, 'ௌ': 46, '்': 47, ' ': 48}\n"
          ]
        }
      ],
      "source": [
        "val_input_characters = set()\n",
        "val_target_characters = set()\n",
        "\n",
        "for i in range(len(x_val)):\n",
        "  val_input_characters.update(list(str(x_val[i])))\n",
        "  val_target_characters.update(list(str(y_val[i])))\n",
        "\n",
        "  # input_characters=list(set(input_characters+list(str(x[i]))))\n",
        "  # target_characters=list(set(target_characters+list(str(y[i]))))\n",
        "\n",
        "val_input_characters = sorted(list(val_input_characters))\n",
        "val_target_characters = sorted(list(val_target_characters))\n",
        "\n",
        "# add the space character to both\n",
        "val_input_characters.append(\" \")\n",
        "val_target_characters.append(\" \")\n",
        "\n",
        "val_num_encoder_tokens = len(val_input_characters)\n",
        "val_num_decoder_tokens = len(val_target_characters)\n",
        "\n",
        "val_input_length=[]\n",
        "val_target_length=[]\n",
        "for i in range(len(x_val)):\n",
        "  val_input_length.append(len(str(x_val[i])))\n",
        "  val_target_length.append(len(str(y_val[i])))\n",
        "\n",
        "val_max_encoder_seq_length = max(val_input_length)\n",
        "val_max_decoder_seq_length = max(val_target_length)\n",
        "\n",
        "print(\"Number of samples in validation set:\", len(x_val))\n",
        "print(\"Number of unique input tokens:\", val_num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", val_num_decoder_tokens)\n",
        "print(\"Max sequence length for validation inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for valiation outputs:\", val_max_decoder_seq_length)\n",
        "\n",
        "val_input_token_index = dict([(char, i) for i, char in enumerate(val_input_characters)])\n",
        "val_target_token_index = dict([(char, i) for i, char in enumerate(val_target_characters)])\n",
        "print(val_input_token_index)\n",
        "print(val_target_token_index)\n",
        "\n",
        "val_encoder_input_data = np.zeros((len(x_val), val_max_encoder_seq_length,val_num_encoder_tokens), dtype=\"float32\")\n",
        "val_decoder_input_data = np.zeros((len(x_val), val_max_decoder_seq_length,val_num_decoder_tokens), dtype=\"float32\")\n",
        "val_decoder_target_data = np.zeros((len(x), max_decoder_seq_length, val_num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(x_val, y_val)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t ,input_token_index[char]] = 1.0\n",
        "    val_encoder_input_data[i, t + 1 :,input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_input_data[i, t ,target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    val_decoder_input_data[i, t + 1 :,target_token_index[\" \"]] = 1.0\n",
        "    val_decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZU-oUHjZsJH",
        "outputId": "bf6d42b3-34cc-4c20-d492-4bae939740d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(68215, 30)\n"
          ]
        }
      ],
      "source": [
        "# Using label encoding for the encoder inputs (and then find an embedding using the Embedding layer)\n",
        "encoder_input_data = np.argmax(encoder_input_data, axis=2)\n",
        "val_encoder_input_data = np.argmax(val_encoder_input_data, axis=2)\n",
        "#test_encoder_input_data = np.argmax(test_encoder_input_data, axis=2)\n",
        "print(encoder_input_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSB81yCrbmow"
      },
      "outputs": [],
      "source": [
        "decoder_input_data = np.argmax(decoder_input_data, axis=2)\n",
        "val_decoder_input_data = np.argmax(val_decoder_input_data, axis=2)\n",
        "#test_decoder_input_data = np.argmax(test_decoder_input_array, axis=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBnkMEHUciI0"
      },
      "outputs": [],
      "source": [
        "#Build the model\n",
        "def create_model(num_encoder_tokens,embedding_size,cell_type,latent_dimension,dropout,number_of_encoder_layers,num_decoder_tokens,number_of_decoder_layers):\n",
        "\n",
        "  # Define an input sequence and process it.\n",
        "  #Add one or more layers at the encoder and decoder side.\n",
        "  encoder_inputs = keras.Input(shape=(None,), name='encoder_input')\n",
        "  encoder = None\n",
        "  encoder_outputs = None\n",
        "  state_h = None\n",
        "  state_c = None\n",
        "  en_embed = tf.keras.layers.Embedding(input_dim=num_encoder_tokens, output_dim=embedding_size,\n",
        "                                            name='encoder_embedding')(encoder_inputs)\n",
        "  if cell_type == 'rnn':\n",
        "      encoder = keras.layers.SimpleRNN(latent_dimension, return_state=True, return_sequences=True,\n",
        "                                       name='encoder_hidden_1', dropout=dropout)\n",
        "      encoder_outputs, state_h = encoder(en_embed)\n",
        "  elif cell_type == 'gru':\n",
        "      encoder = keras.layers.GRU(latent_dimension, return_state=True, return_sequences=True,\n",
        "                                  name='encoder_hidden_1', dropout=dropout)\n",
        "      encoder_outputs, state_h = encoder(en_embed)\n",
        "  else:\n",
        "      encoder = keras.layers.LSTM(latent_dimension, return_state=True, return_sequences=True,\n",
        "                                  name='encoder_hidden_1', dropout=dropout)\n",
        "      encoder_outputs, state_h, state_c = encoder(en_embed)\n",
        "\n",
        "  # First encoder layer\n",
        "\n",
        "\n",
        "  e_layer = number_of_encoder_layers\n",
        "  for i in range(2, e_layer + 1):\n",
        "    # Give the output sequences as input to the next layer \n",
        "    \n",
        "    # Last state is set as initial state of next layer\n",
        "    layer_name = ('encoder_hidden_%d') % i\n",
        "    if cell_type == 'rnn':\n",
        "      encoder = keras.layers.SimpleRNN(latent_dimension, return_state=True, return_sequences=True,\n",
        "                                          name=layer_name, dropout=dropout)\n",
        "      encoder_outputs, state_h = encoder(encoder_outputs, initial_state=[state_h])\n",
        "    elif cell_type == 'gru':\n",
        "        encoder = keras.layers.GRU(latent_dimension, return_state=True, return_sequences=True,\n",
        "                                    name=layer_name, dropout=dropout)\n",
        "        encoder_outputs, state_h = encoder(encoder_outputs, initial_state=[state_h])\n",
        "    else:\n",
        "        encoder = keras.layers.LSTM(latent_dimension, return_state=True, return_sequences=True,\n",
        "                                    name=layer_name, dropout=dropout)\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_outputs, initial_state=[state_h, state_c])\n",
        "  \n",
        "  encoder_states = None\n",
        "\n",
        "  # saving the last state\n",
        "  if cell_type  == 'rnn' or cell_type == 'gru' :\n",
        "      encoder_states = [state_h]\n",
        "  else:\n",
        "      encoder_states = [state_h, state_c]\n",
        "  decoder_inputs = keras.Input(shape=(None,), name='decoder_input')\n",
        "  de_embed = tf.keras.layers.Embedding(num_decoder_tokens, embedding_size, name='decoder_embedding')(decoder_inputs)\n",
        " # number of decoder layers\n",
        "  d_layer = number_of_decoder_layers\n",
        "  decoder = None\n",
        "\n",
        "  # first decoder layer\n",
        "\n",
        "  if cell_type == 'rnn':\n",
        "      decoder = keras.layers.SimpleRNN(latent_dimension, return_sequences=True, return_state=True,\n",
        "                                        name='decoder_hidden_1', dropout=dropout)\n",
        "      \n",
        "     # For all decoder layers, the initial state is the last state of the last encoder layer\n",
        "      decoder_outputs, _ = decoder(de_embed, initial_state=encoder_states)\n",
        "  elif cell_type == 'gru':\n",
        "      decoder = keras.layers.GRU(latent_dimension, return_sequences=True, return_state=True,\n",
        "                                  name='decoder_hidden_1', dropout=dropout)\n",
        "      \n",
        "      decoder_outputs, _ = decoder(de_embed, initial_state=encoder_states)\n",
        "  else:\n",
        "      decoder = keras.layers.LSTM(latent_dimension, return_sequences=True, return_state=True,\n",
        "                                  name='decoder_hidden_1', dropout=dropout)\n",
        "      \n",
        "      decoder_outputs, _, _ = decoder(de_embed, initial_state=encoder_states)\n",
        "\n",
        "  for i in range(2, d_layer + 1):\n",
        "      layer_name = 'decoder_hidden_%d' % i\n",
        "      if cell_type == 'rnn':\n",
        "          decoder = keras.layers.SimpleRNN(latent_dimension, return_sequences=True, return_state=True,\n",
        "                                            name=layer_name, dropout=dropout)\n",
        "          decoder_outputs, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
        "      elif cell_type == 'gru':\n",
        "          decoder = keras.layers.GRU(latent_dimension, return_sequences=True, return_state=True,\n",
        "                                      name=layer_name, dropout=dropout)\n",
        "          decoder_outputs, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
        "      else:\n",
        "          decoder = keras.layers.LSTM(latent_dimension, return_sequences=True, return_state=True,\n",
        "                                      name=layer_name, dropout=dropout)\n",
        "          decoder_outputs, _, _ = decoder(decoder_outputs, initial_state=encoder_states)\n",
        "\n",
        "  # add a dense layer\n",
        "  decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\", name='decoder_output')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  \n",
        "  return model\n",
        "  # # We discard `encoder_outputs` and only keep the states.\n",
        "  # encoder_states = [state_h, state_c]\n",
        "\n",
        "  # # Set up the decoder, using `encoder_states` as initial state.\n",
        "  # decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "  # # We set up our decoder to return full output sequences,\n",
        "  # # and to return internal states as well. We don't use the\n",
        "  # # return states in the training model, but we will use them in inference.\n",
        "  # decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "  # decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "  # decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "  # decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  # # Define the model that will turn\n",
        "  # # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "  # model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSK2SsA_L5k2"
      },
      "outputs": [],
      "source": [
        "def fit(model,cell_type,encoder_input_data, decoder_input_data, decoder_target_data,batch_size, epochs,number_of_encoder_layers,number_of_decoder_layers,latent_dimension, callbacks=None):\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "  model.fit(\n",
        "      [encoder_input_data, decoder_input_data],\n",
        "      decoder_target_data,\n",
        "      batch_size=batch_size,\n",
        "      epochs=epochs,\n",
        "      callbacks=callbacks\n",
        "  )\n",
        "\n",
        "  # create inference model\n",
        "  encoder_inputs = model.input[0]  # input_1\n",
        "  if cell_type == 'rnn' or cell_type == 'gru':\n",
        "      encoder_outputs, state_h_enc = model.get_layer('encoder_hidden_' + str(number_of_encoder_layers)).output\n",
        "      encoder_states = [state_h_enc]\n",
        "      encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "      decoder_inputs = model.input[1]  # input_2\n",
        "      decoder_outputs = model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for j in range(1, number_of_decoder_layers + 1):\n",
        "          decoder_state_input_h = keras.Input(shape=(latent_dimension,))\n",
        "          current_states_inputs = [decoder_state_input_h]\n",
        "          decoder = model.get_layer('decoder_hidden_' + str(j))\n",
        "          decoder_outputs, state_h_dec = decoder(decoder_outputs, initial_state=current_states_inputs)\n",
        "          decoder_states += [state_h_dec]\n",
        "          decoder_states_inputs += current_states_inputs\n",
        "  else:\n",
        "      encoder_outputs, state_h_enc, state_c_enc = model.get_layer('encoder_hidden_'+ str(number_of_encoder_layers)).output\n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "      encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "      decoder_inputs = model.input[1]  # input_2\n",
        "      decoder_outputs = model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for j in range(1,number_of_decoder_layers + 1):\n",
        "          decoder_state_input_h = keras.Input(shape=(latent_dimension,))\n",
        "          decoder_state_input_c = keras.Input(shape=(latent_dimension,))\n",
        "          current_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "          decoder = model.get_layer('decoder_hidden_' + str(j))\n",
        "          decoder_outputs, state_h_dec, state_c_dec = decoder(decoder_outputs, initial_state=current_states_inputs)\n",
        "          decoder_states += [state_h_dec, state_c_dec]\n",
        "          decoder_states_inputs += current_states_inputs\n",
        "          \n",
        "  decoder_dense = model.get_layer('decoder_output')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "  return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6tGAC4QQzbY"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences of numbers back to sequence of characters.\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_7DphZFRR_4"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq,number_of_decoder_layers,target_token_index,cell_type,\n",
        "                    reverse_target_char_index,max_decoder_seq_length,\n",
        "                    encoder_model,decoder_model):\n",
        "  # Encode the input as state vectors.\n",
        "  states_value = [encoder_model.predict(input_seq)]*number_of_decoder_layers\n",
        "\n",
        "  # Generate empty target sequence of length 1.\n",
        "  empty_seq = np.zeros((1, 1))\n",
        "\n",
        "  # Populate the first character of target sequence with the start character.\n",
        "  empty_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "  target_seq = empty_seq\n",
        "\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_sentence = \"\"\n",
        "  while not stop_condition:\n",
        "      if cell_type  == 'rnn' or cell_type == 'gru':\n",
        "          temp = decoder_model.predict([target_seq] + [states_value])\n",
        "          output_tokens, states_value = temp[0], temp[1:]\n",
        "      else:\n",
        "          temp = decoder_model.predict([target_seq] + states_value )\n",
        "          output_tokens, states_value = temp[0], temp[1:]\n",
        "\n",
        "      # Sample a token\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "      sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "      decoded_sentence += sampled_char\n",
        "\n",
        "      # Exit condition: Encounter stop character or be greater than the max decoder length\n",
        "      \n",
        "      if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "          stop_condition = True\n",
        "\n",
        "      # Update the target sequence (of length 1).\n",
        "      target_seq = np.zeros((1, 1))\n",
        "      target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "  return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBAGbqsgRmgH"
      },
      "outputs": [],
      "source": [
        "def accuracy(val_encoder_input_data, y_val,number_of_decoder_layers,target_token_index,cell_type,\n",
        "                    reverse_target_char_index,max_decoder_seq_length,\n",
        "                    encoder_model,decoder_model,verbose=False):\n",
        "  n_correct = 0\n",
        "  n_total = 0\n",
        "  number_of_decoder_layers=number_of_decoder_layers\n",
        "  for seq_index in range(len(val_encoder_input_data)):\n",
        "\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = val_encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq,number_of_decoder_layers,target_token_index,cell_type,\n",
        "                    reverse_target_char_index,max_decoder_seq_length,\n",
        "                    encoder_model,decoder_model)\n",
        "    # print(\"-\")\n",
        "    # print(\"Input sentence:\", y_val[seq_index])\n",
        "    # print(\"Decoded sentence:\", decoded_sentence)\n",
        "\n",
        "    if decoded_sentence.strip() == y_val[seq_index].strip():\n",
        "        n_correct += 1\n",
        "\n",
        "    n_total += 1\n",
        "\n",
        "    if verbose:\n",
        "        print('Prediction ', decoded_sentence.strip(), ',Ground Truth ', y_val[seq_index].strip())\n",
        "\n",
        "  return n_correct * 100.0 / n_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV3IKW99gWes"
      },
      "outputs": [],
      "source": [
        "#SWEEP\n",
        "def sweep():\n",
        "  run = wandb.init()\n",
        "  config = wandb.config\n",
        "  name = ('cell_type=%s' % config.cell_type) + \"_\" \\\n",
        "          + ('embedding=%d' % config.embedding_size) + \"_\" \\\n",
        "          + ('n_enc_layers=%d' % config.number_of_encoder_layers) + \"_\" \\\n",
        "          + ('n_dec_layers=%d' % config.number_of_decoder_layers) + \"_\" \\\n",
        "          + ('latent=%d' % config.latent_dimension) + \"_\" \\\n",
        "          + ('dropout=%.2f' % config.dropout) + \"_\" \\\n",
        "          + ('epochs=%d' % config.epochs)\n",
        "\n",
        "  wandb.run.name = name\n",
        "  batch_size = 64  # Batch size for training.\n",
        "  epochs = config.epochs  # Number of epochs to train for.\n",
        "  latent_dim = config.latent_dimension  # Latent dimensionality of the encoding space .#hidden states hyperparameter\n",
        "\n",
        "  #Train the model\n",
        "  model = create_model(num_encoder_tokens=num_encoder_tokens,embedding_size=config.embedding_size,\n",
        "                      cell_type = config.cell_type,latent_dimension=config.latent_dimension,\n",
        "                      dropout=config.dropout,number_of_encoder_layers = config.number_of_encoder_layers,\n",
        "                      num_decoder_tokens=num_decoder_tokens,\n",
        "                      number_of_decoder_layers=config.number_of_decoder_layers)\n",
        "  \n",
        "  encoder_model , decoder_model = fit(model=model,cell_type=config.cell_type,encoder_input_data=encoder_input_data, \n",
        "                                    decoder_input_data=decoder_input_data,\n",
        "          decoder_target_data=decoder_target_data,batch_size=batch_size,epochs=config.epochs,\n",
        "          number_of_encoder_layers = config.number_of_encoder_layers,\n",
        "          number_of_decoder_layers= config.number_of_decoder_layers,latent_dimension=latent_dim, \n",
        "          callbacks=[WandbCallback()])\n",
        "  #Validation accuracy\n",
        "\n",
        "  subset = 20\n",
        "\n",
        "  val_accuracy = accuracy(val_encoder_input_data, y_val,\n",
        "                          number_of_decoder_layers=config.number_of_decoder_layers,\n",
        "                          target_token_index=target_token_index,cell_type=config.cell_type,\n",
        "                          reverse_target_char_index=reverse_target_char_index,max_decoder_seq_length=max_decoder_seq_length,\n",
        "                          encoder_model=encoder_model,decoder_model=decoder_model\n",
        "                          )\n",
        "  print('Validation accuracy: ', val_accuracy)\n",
        "  wandb.log({'val_accuracy': val_accuracy})\n",
        "  run.finish()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcddoYuTPa0Z"
      },
      "outputs": [],
      "source": [
        "# run sweeps\n",
        "sweep_config = {\n",
        "    'method': 'bayes',  # grid, random\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [20,25,30]\n",
        "        },\n",
        "        'embedding_size': {\n",
        "            'values': [16,32,64,128,256]\n",
        "        },\n",
        "        'number_of_encoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'number_of_decoder_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        'latent_dimension': {\n",
        "            'values': [16, 32, 64, 256, 512]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['gru','lstm','rnn']\n",
        "        },                             \n",
        "        'dropout': {\n",
        "            'values': [0.0,0.2,0.3,0.4,0.5]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "nqIfTWWLqgoX",
        "outputId": "115b0fa5-b4bd-4214-80e3-1b8630d0c9df"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 58r6tft4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlatent_dimension: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_decoder_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_encoder_layers: 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.15"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220430_065312-58r6tft4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil/runs/58r6tft4\" target=\"_blank\">driven-sweep-100</a></strong> to <a href=\"https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil/sweeps/emcrqbu0\" target=\"_blank\">https://wandb.ai/hithesh-sidhesh/CS6910-Assignment_3-sweep-Tamil/sweeps/emcrqbu0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1066/1066 [==============================] - 60s 52ms/step - loss: 1.0760 - accuracy: 0.7403 - _timestamp: 1651301659.0000 - _runtime: 67.0000\n",
            "Epoch 2/20\n",
            "1066/1066 [==============================] - 55s 52ms/step - loss: 0.7425 - accuracy: 0.7895 - _timestamp: 1651301715.0000 - _runtime: 123.0000\n",
            "Epoch 3/20\n",
            "1066/1066 [==============================] - 55s 52ms/step - loss: 0.6812 - accuracy: 0.8061 - _timestamp: 1651301770.0000 - _runtime: 178.0000\n",
            "Epoch 4/20\n",
            "1066/1066 [==============================] - 55s 51ms/step - loss: 0.6476 - accuracy: 0.8157 - _timestamp: 1651301824.0000 - _runtime: 232.0000\n",
            "Epoch 5/20\n",
            "1066/1066 [==============================] - 55s 51ms/step - loss: 0.6293 - accuracy: 0.8198 - _timestamp: 1651301879.0000 - _runtime: 287.0000\n",
            "Epoch 6/20\n",
            "1066/1066 [==============================] - 55s 51ms/step - loss: 0.6169 - accuracy: 0.8232 - _timestamp: 1651301934.0000 - _runtime: 342.0000\n",
            "Epoch 7/20\n",
            "1066/1066 [==============================] - 55s 51ms/step - loss: 0.6068 - accuracy: 0.8261 - _timestamp: 1651301988.0000 - _runtime: 396.0000\n",
            "Epoch 8/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5985 - accuracy: 0.8284 - _timestamp: 1651302043.0000 - _runtime: 451.0000\n",
            "Epoch 9/20\n",
            "1066/1066 [==============================] - 55s 51ms/step - loss: 0.5918 - accuracy: 0.8306 - _timestamp: 1651302097.0000 - _runtime: 505.0000\n",
            "Epoch 10/20\n",
            "1066/1066 [==============================] - 55s 51ms/step - loss: 0.5860 - accuracy: 0.8324 - _timestamp: 1651302152.0000 - _runtime: 560.0000\n",
            "Epoch 11/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5811 - accuracy: 0.8337 - _timestamp: 1651302206.0000 - _runtime: 614.0000\n",
            "Epoch 12/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5770 - accuracy: 0.8350 - _timestamp: 1651302260.0000 - _runtime: 668.0000\n",
            "Epoch 13/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5732 - accuracy: 0.8361 - _timestamp: 1651302315.0000 - _runtime: 723.0000\n",
            "Epoch 14/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5704 - accuracy: 0.8368 - _timestamp: 1651302369.0000 - _runtime: 777.0000\n",
            "Epoch 15/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5672 - accuracy: 0.8376 - _timestamp: 1651302423.0000 - _runtime: 831.0000\n",
            "Epoch 16/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5649 - accuracy: 0.8383 - _timestamp: 1651302477.0000 - _runtime: 885.0000\n",
            "Epoch 17/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5625 - accuracy: 0.8388 - _timestamp: 1651302532.0000 - _runtime: 940.0000\n",
            "Epoch 18/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5605 - accuracy: 0.8394 - _timestamp: 1651302586.0000 - _runtime: 994.0000\n",
            "Epoch 19/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5587 - accuracy: 0.8400 - _timestamp: 1651302640.0000 - _runtime: 1048.0000\n",
            "Epoch 20/20\n",
            "1066/1066 [==============================] - 54s 51ms/step - loss: 0.5566 - accuracy: 0.8404 - _timestamp: 1651302694.0000 - _runtime: 1102.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "#start wandb sweep\n",
        "sweep_id = \"emcrqbu0\"\n",
        "wandb.agent(sweep_id, sweep, project=\"CS6910-Assignment_3-sweep-Tamil\",entity=\"hithesh-sidhesh\",count = 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN5-jep3Ssxg"
      },
      "outputs": [],
      "source": [
        "# sweep_id = wandb.sweep(sweep_config, project=\"CS6910-Assignment_3-sweep-Tamil\")\n",
        "# #start wandb sweep\n",
        "# wandb.agent(sweep_id, sweep, project=\"CS6910-Assignment_3-sweep-Tamil\",count = 2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy of ASG3_Q2_sweep_final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1xinvsbYk9KOMGvQYtnKX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}